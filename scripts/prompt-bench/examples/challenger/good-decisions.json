[
  {
    "id": "challenge-001",
    "description": "Well-scoped sprint plan within velocity — should approve",
    "decision": "Sprint 7 Plan: We're pulling in 5 issues totaling 18 story points, matching our 3-sprint rolling average velocity of 19 points. The sprint goal is 'Complete user profile settings.' Issues are prioritized by ICE score: Profile avatar upload (8pts), Email notification preferences (5pts), Password change flow (3pts), and two small UX tweaks (1pt each). All issues have acceptance criteria defined and dependencies mapped. No carryover from Sprint 6.",
    "context": "Team velocity: Sprint 4=17pts, Sprint 5=20pts, Sprint 6=20pts. All Sprint 6 issues completed. No major holidays or PTO planned. Tech stack is stable with no pending upgrades.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK", "scope creep"]
    }
  },
  {
    "id": "challenge-002",
    "description": "Architectural decision with clear tradeoffs — should approve",
    "decision": "ADR-012: We propose using PostgreSQL over MongoDB for the new analytics service. Tradeoffs: PostgreSQL gives us ACID transactions for financial reporting accuracy but requires schema migrations. MongoDB would offer flexible schemas but makes cross-collection joins complex. We've benchmarked both: PostgreSQL handles our projected 50k queries/day with p99 < 100ms. The team has 3 years PostgreSQL experience vs 6 months MongoDB. Migration path: start with a single analytics schema, expand tables as reporting needs grow. Rollback: if PostgreSQL doesn't meet performance targets within 2 sprints, we can pivot to a hybrid approach with MongoDB for raw event storage.",
    "context": "Building a new analytics dashboard. Current stack is PostgreSQL for the main app. Team of 4 backend engineers, all experienced with SQL. Expected data volume: ~2GB/month initially, growing to ~20GB/month within a year.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-003",
    "description": "Bug fix with regression test plan — should approve",
    "decision": "Bug Fix Plan for Issue #234 (Race Condition in Session Manager): Root cause identified — the session refresh token rotation has a TOCTOU race where two concurrent requests can both read the same refresh token before either writes the new one, resulting in one request getting a stale token. Fix approach: Add an optimistic locking mechanism using a version counter on the refresh token row. If the version doesn't match at write time, retry the rotation. Regression test plan: (1) Unit test for concurrent token rotation with 10 simultaneous requests, (2) Integration test verifying that a stale token triggers retry, (3) Load test with 100 concurrent users to confirm no regressions under stress. Rollback: revert the migration and code change; old behavior is 'last write wins' which is the current broken state.",
    "context": "This bug affects ~2% of users during peak hours. It was reported 3 weeks ago and has been triaged as P1. The session manager is well-tested otherwise with 94% code coverage.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-004",
    "description": "Database migration with rollback strategy — should approve",
    "decision": "Migration Plan: Adding a 'preferences' JSONB column to the users table. Step 1: Add nullable column with default '{}' (no downtime). Step 2: Backfill existing user preferences from the legacy preferences table using a background job (~2 hours for 500k rows, batched in 1000-row chunks). Step 3: Update application code to read/write from new column (feature-flagged). Step 4: After 2 weeks of dual-write verification, drop the legacy table. Rollback at each step: Step 1 — drop column. Step 2 — stop backfill job, no data loss. Step 3 — toggle feature flag off. Step 4 — restore from backup (tested monthly). We'll run the migration during low-traffic hours (2-4 AM UTC) with a maintenance window announcement 48 hours in advance.",
    "context": "PostgreSQL 15, 500k users, 99.9% uptime SLA. Last migration (3 weeks ago) went smoothly. DBA reviewed the plan and signed off. Staging environment migration completed in 45 minutes.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-005",
    "description": "Incremental refactor with feature flag — should approve",
    "decision": "Refactoring Proposal: Extract the payment processing logic from the monolithic OrderService into a dedicated PaymentService. This is Phase 1 of 3, focused only on extracting the charge/refund methods. We'll use the Strangler Fig pattern: new PaymentService handles all new payment calls, old OrderService methods are deprecated but remain functional. A feature flag ('use-payment-service') controls the rollout — 10% canary → 50% → 100% over 3 sprints. Each phase has its own PR with tests. We are NOT changing the payment gateway integration, NOT modifying the API surface, and NOT touching the billing pipeline. Those are separate issues if needed.",
    "context": "OrderService is 2,400 lines and handles orders, payments, shipping, and notifications. Tech debt issue has been in backlog for 6 months. Current test coverage for payment methods: 87%.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK", "scope"]
    }
  },
  {
    "id": "challenge-006",
    "description": "Security patch with minimal scope — should approve",
    "decision": "Security Patch: Upgrade jsonwebtoken from 8.5.1 to 9.0.2 to address CVE-2022-23529 (prototype pollution in jwt.verify). Change scope: update package.json and package-lock.json, update one call site where we pass algorithm options explicitly (the new version requires 'algorithms' as an array, not a string). Test plan: run existing auth test suite (42 tests), add one new test for the algorithm array format. No API changes, no behavior changes for end users. Deployment: standard CI/CD pipeline, no special rollout needed since the fix is backward-compatible at the API level.",
    "context": "Vulnerability scanner flagged this as HIGH severity. The vulnerable function is called on every authenticated API request (~10k/hour). No known exploits in the wild targeting our specific usage pattern, but the CVE is public.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-007",
    "description": "Well-planned API versioning strategy — should approve",
    "decision": "API Versioning Proposal: We'll adopt URL-based versioning (e.g., /api/v2/users) for the upcoming breaking changes to the user endpoint. Plan: (1) v1 endpoints remain unchanged and supported for 6 months after v2 launch. (2) v2 endpoints are developed in parallel using a version router middleware. (3) All v1 consumers get deprecation headers starting Sprint 8. (4) Usage analytics track v1 vs v2 adoption weekly. (5) v1 sunset date announced 3 months in advance via changelog, email, and in-app notification. Migration guide will be published alongside v2 launch. We've identified 3 external consumers and 12 internal services using v1 — each will get a dedicated migration ticket.",
    "context": "Current API has been stable for 18 months. The breaking changes are needed to support multi-tenant user accounts (requested by 40% of enterprise customers). We have full control over all internal consumers.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-008",
    "description": "Test infrastructure improvement with clear metrics — should approve",
    "decision": "Proposal: Add integration test suite using Testcontainers for database-dependent tests. Currently we mock the database layer in 60% of our service tests, which has led to 3 production bugs in the last quarter where mocks diverged from actual DB behavior. Plan: (1) Add Testcontainers as a dev dependency. (2) Convert the 15 highest-risk service tests from mocks to real PostgreSQL containers. (3) Keep unit tests with mocks for pure logic. (4) CI pipeline gets a new 'integration' stage that runs after unit tests. Expected impact: CI time increases from 3 min to 5 min (Testcontainers reuse). Success metric: zero mock-divergence bugs in the next 2 quarters.",
    "context": "Team has been discussing this for 2 sprints. The 3 mock-divergence bugs each took 1-2 days to diagnose. CI budget allows up to 8 minutes per run. Testcontainers is already used by the platform team.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-009",
    "description": "Monitoring and alerting plan with clear thresholds — should approve",
    "decision": "Monitoring Plan for Payment Service: Adding structured alerting with three tiers. Tier 1 (P1, page on-call): payment failure rate > 5% over 5-minute window, or payment latency p99 > 3 seconds. Tier 2 (P2, Slack alert): payment failure rate > 2% over 15-minute window, or daily transaction volume drops > 20% from 7-day average. Tier 3 (P3, daily digest): any payment provider returning non-200 status codes, retry queue depth > 100. Implementation: Datadog monitors with existing infrastructure — no new vendors. Runbooks for each Tier 1 alert already drafted. On-call rotation covers 24/7 with 2 engineers minimum. Alert fatigue mitigation: monthly review of alert frequency, auto-silence flapping alerts after 3 occurrences.",
    "context": "Payment service launched 4 weeks ago. Currently zero alerting — team manually checks dashboards. One incident last week went unnoticed for 45 minutes. On-call rotation has 6 engineers.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-010",
    "description": "Dependency upgrade with compatibility testing — should approve",
    "decision": "Proposal: Upgrade Node.js from 18 LTS to 20 LTS. Preparation completed: (1) Read the Node.js 20 changelog — no breaking changes affect our codebase. (2) Ran full test suite on Node 20 in CI — all 347 tests pass. (3) Benchmarked key endpoints — 8% improvement in response times due to V8 improvements. (4) All 12 npm dependencies are compatible (verified via npm ls with --engine-strict). (5) Docker base image updated in staging, running for 1 week with zero issues. Rollout: update CI matrix to include Node 20, run both 18 and 20 for 1 sprint, then drop 18. Rollback: revert Dockerfile base image tag.",
    "context": "Node 18 LTS EOL is April 2025. Node 20 LTS supported until April 2026. All team members have Node 20 installed locally. Staging has been running Node 20 for 7 days with production-like traffic.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-011",
    "description": "Clear deprecation plan with timeline — should approve",
    "decision": "Deprecation Plan for Legacy Webhook System: Phase 1 (Sprint 8): Add deprecation warnings to all legacy webhook API responses via 'Sunset' HTTP header. Phase 2 (Sprint 9-10): Build new webhook system with retry logic, payload signing, and event filtering. Phase 3 (Sprint 11): Dual-run both systems, new webhooks default for new consumers. Phase 4 (Sprint 14): Send final deprecation notice to 8 remaining legacy consumers (identified via usage logs). Phase 5 (Sprint 16): Disable legacy system, keep code for 2 more sprints as emergency fallback. Each consumer gets a migration guide and 1:1 support session if requested. Zero legacy consumers should remain by Sprint 16.",
    "context": "Legacy webhook system was built 2 years ago, has no retry logic, no payload signing, and has caused 5 incidents in the last quarter due to failed deliveries. 23 consumers total, 15 already on the new system.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-012",
    "description": "Performance optimization with benchmarks — should approve",
    "decision": "Performance Optimization: Replace the N+1 query pattern in the dashboard endpoint. Current state: loading a user's dashboard makes 1 query for the user + N queries for their projects (avg 12 projects per user). Proposed fix: single query with LEFT JOIN and application-level grouping. Benchmark results from staging: response time drops from 340ms (p50) to 45ms (p50), database load drops 85%. Implementation: modify the DashboardRepository.getUserDashboard() method, update the result mapper. No API changes — response shape is identical. Test plan: existing integration tests cover the response shape; add a new benchmark test asserting < 100ms p99 for users with 50 projects.",
    "context": "Dashboard endpoint is the most-hit endpoint (40% of all traffic). Current p99 is 1.2 seconds, which triggers our SLO warning threshold. Database connection pool sometimes exhausts during peak hours.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-013",
    "description": "Well-structured error handling improvement — should approve",
    "decision": "Error Handling Improvement: Introduce a structured error hierarchy for the API layer. Currently, we throw generic Error objects with message strings, which makes error handling in clients inconsistent. Proposal: Create an AppError base class with code, statusCode, and isOperational properties. Subclasses: ValidationError (400), AuthenticationError (401), AuthorizationError (403), NotFoundError (404), ConflictError (409). The global error handler maps these to consistent JSON responses. Existing throw statements (47 total) will be migrated incrementally over 2 sprints. No changes to successful response formats. Error response format: { error: { code, message, details? } }.",
    "context": "Client team has requested consistent error formats for 3 months. Current error responses vary between { message }, { error }, and { errors: [] } depending on which service handles the error. 8 different error response shapes identified in production.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-014",
    "description": "CI pipeline optimization with measurable goals — should approve",
    "decision": "CI Pipeline Optimization: Current pipeline takes 14 minutes. Target: under 8 minutes. Changes: (1) Parallelize lint, typecheck, and unit tests (currently sequential) — saves ~3 min. (2) Cache node_modules between runs using CI cache key on package-lock.json hash — saves ~1.5 min. (3) Split integration tests into 3 parallel shards — saves ~2 min. (4) Remove duplicate TypeScript compilation (build step already does tsc, no need for separate tsc --noEmit) — saves ~45 sec. No test removal, no coverage reduction. Each optimization is a separate PR that can be reverted independently. Success metric: p50 pipeline duration < 8 minutes measured over 1 week.",
    "context": "Team runs CI ~40 times per day. At 14 min per run, that's 9+ hours of CI time daily. Developers report context-switching while waiting for CI. CI cost is within budget even with parallelization.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-015",
    "description": "Feature flag cleanup with safety checks — should approve",
    "decision": "Feature Flag Cleanup: Remove 12 stale feature flags that have been at 100% rollout for over 90 days. For each flag: (1) Verify 100% rollout in LaunchDarkly for > 90 days. (2) Search codebase for all references. (3) Remove conditional branches, keeping only the 'on' path. (4) Remove flag from LaunchDarkly after code merges. (5) Each flag removal is a separate PR with its own tests. Flags to remove: 'new-checkout-flow' (180 days), 'v2-search' (120 days), 'dark-mode' (95 days), and 9 others. Not removing any flags under 90 days or any that are partially rolled out.",
    "context": "LaunchDarkly currently has 47 feature flags. 12 are stale (100% for 90+ days). Flag evaluation adds ~2ms latency per request. Codebase has accumulated 34 dead conditional branches from old flags.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-016",
    "description": "Capacity planning with data-driven projections — should approve",
    "decision": "Capacity Planning for Q3: Based on current growth rate (15% MoM) and infrastructure metrics, we project needing: (1) Scale database from db.r6g.xlarge to db.r6g.2xlarge by month 2 of Q3 — current CPU utilization at 68% peak, projected to hit 85% by then. (2) Add 2 more application server instances to the auto-scaling group — current max is 6, we've hit 5 twice in the last month. (3) Increase Redis cache from 13GB to 26GB — eviction rate has been climbing 5% weekly. Cost impact: +$1,200/month. Timeline: database upgrade in Sprint 9 (requires 5-minute maintenance window), app servers and Redis are zero-downtime changes. All changes are reversible.",
    "context": "SaaS product with 12k active users, growing 15% monthly for the last 6 months. No major marketing campaigns planned for Q3. Current infrastructure costs: $4,800/month. Budget approved for up to $7,000/month.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-017",
    "description": "Documentation-first approach for new feature — should approve",
    "decision": "Documentation-First Approach for Webhooks V2: Before writing any code, we'll create: (1) API specification in OpenAPI 3.1 format — reviewed by frontend and partner teams. (2) Event catalog listing all 15 event types with payload schemas. (3) Integration guide with code examples in JavaScript, Python, and curl. (4) Architecture diagram showing event flow from trigger to delivery. Review process: API spec reviewed in Sprint 8 planning, documentation reviewed by developer relations in Sprint 8 week 1, implementation starts Sprint 8 week 2. This front-loads the design feedback and prevents mid-implementation API changes.",
    "context": "Webhooks V1 suffered from poor documentation — 60% of support tickets were 'how do I use webhooks?' questions. Partner team requested involvement in API design before implementation starts.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-018",
    "description": "Thoughtful team process improvement — should approve",
    "decision": "Process Improvement: Introduce 15-minute daily async standups via Slack bot, replacing 30-minute synchronous standups. Format: each team member posts (1) what they completed, (2) what they're working on, (3) blockers — by 10 AM. If blockers exist, a synchronous huddle is auto-scheduled for 10:30 AM with only affected parties. Trial period: 4 weeks (2 sprints). Success metrics: (a) time saved per person per week > 1 hour, (b) blocker resolution time stays the same or improves, (c) team satisfaction survey score ≥ 4/5. If metrics aren't met, we revert to synchronous standups.",
    "context": "Team of 6 across 3 time zones (UTC-5, UTC+0, UTC+8). Current 30-min standup at 9 AM UTC means 4 AM for one team member and 5 PM for another. Team satisfaction with current standups: 2.8/5.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-019",
    "description": "Gradual rollout plan with canary deployment — should approve",
    "decision": "Rollout Plan for New Search Algorithm: Phase 1 (1 week): 5% of traffic via canary deployment, monitoring search relevance scores, click-through rates, and latency. Phase 2 (1 week): 25% rollout if Phase 1 metrics are neutral or positive. Phase 3 (1 week): 50% rollout with A/B test comparing old vs new algorithm. Phase 4: 100% rollout if A/B shows ≥ 5% improvement in click-through rate with no latency regression. Kill switch: instant rollback via feature flag, tested in staging. Monitoring: dedicated Grafana dashboard with search-specific metrics, alerting if relevance score drops > 10% in any 1-hour window.",
    "context": "Search is the primary user flow (70% of sessions start with a search). Current algorithm was implemented 18 months ago. New algorithm uses TF-IDF scoring instead of simple keyword matching. Staging tests show 12% improvement in relevance scores.",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  },
  {
    "id": "challenge-020",
    "description": "Incident response improvement with clear actions — should approve",
    "decision": "Post-Incident Improvement Plan (from Incident #47 retrospective): Three action items, each with an owner and deadline. (1) Add circuit breaker to payment gateway calls — if error rate > 10% in 30 seconds, stop sending requests and return cached payment status. Owner: Sarah, Sprint 8. (2) Add payment service health check to the status page — currently only shows API and database status. Owner: Mike, Sprint 8. (3) Create runbook for 'payment gateway degraded' scenario with step-by-step triage guide. Owner: Alex, Sprint 8 week 1. All three items are already in the backlog with acceptance criteria. No process changes proposed — our incident response time (12 minutes to acknowledge) was within SLA.",
    "context": "Incident #47: Payment gateway had 15-minute outage, our system kept retrying and exhausted the connection pool, causing cascading failure. Total downtime: 35 minutes. Customer impact: ~200 failed transactions (all were retried successfully after recovery).",
    "expected": {
      "passed": true,
      "mustContain": ["APPROVE"],
      "mustNotContain": ["PUSH_BACK"]
    }
  }
]
